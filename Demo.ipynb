{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "f3fa9ece-fb8a-45ed-9d22-231b7668651d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "\n",
    "from kfp.dsl import Input\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import Dataset\n",
    "from kfp.dsl import Model\n",
    "from kfp.dsl import HTML\n",
    "from kfp.dsl import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "0184c927-417d-4633-b70a-463c80b1dd38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=[\"huggingface_hub\"])\n",
    "def download_model_pretrained(hug_model_name: str, hug_model_revision: str, llm_model_pretrained: Output[Model]):\n",
    "    import os  \n",
    "    import zipfile\n",
    "\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger('kfp_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    def zip_folder(folder, zip_name):  \n",
    "        # Create a ZipFile object  \n",
    "        with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:  \n",
    "            # Walk through all files and directories in the specified folder  \n",
    "            for root, dirs, files in os.walk(folder):  \n",
    "                for file in files:  \n",
    "                    # Create the complete file path  \n",
    "                    file_path = os.path.join(root, file)  \n",
    "                    # Add the file to the ZIP file, preserving the folder structure  \n",
    "                    zipf.write(file_path, os.path.relpath(file_path, os.path.dirname(folder)))  \n",
    "\n",
    "    logger.info(f\"{hug_model_name} {hug_model_revision}\")\n",
    "                    \n",
    "    model_name     = os.path.basename(hug_model_name)\n",
    "    model_user_hug = os.path.dirname (hug_model_name)\n",
    "    \n",
    "    logger.info(f\"{model_user_hug} {model_name}\")\n",
    "                    \n",
    "    os.makedirs(f\"/tmp/{model_name}\", exist_ok=True)\n",
    "    \n",
    "    allow_patterns = [\n",
    "        \"*.json\",\n",
    "        \"*.safetensors\",\n",
    "    ]\n",
    "\n",
    "    snapshot_path = snapshot_download(\n",
    "        repo_id=hug_model_name,\n",
    "        revision=hug_model_revision,\n",
    "        allow_patterns=allow_patterns,\n",
    "        cache_dir=f\"/tmp/{model_name}\",\n",
    "        use_auth_token=False,\n",
    "    )\n",
    "    \n",
    "    model_dir = llm_model_pretrained.path\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    llm_model_pretrained.path                  = os.path.join(model_dir, f\"{model_name}.zip\")\n",
    "    llm_model_pretrained.metadata[\"revision\"]  = hug_model_revision\n",
    "    \n",
    "    zip_folder(f\"/tmp/{model_name}/models--{model_user_hug}--{model_name}/snapshots/{hug_model_revision}\", llm_model_pretrained.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "835f8616-999a-4cf3-9a47-92cf824e2b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=[\n",
    "        \"torch\", \"transformers\", \"peft\", \"scikit-learn\",\n",
    "        \"datasets\", \"tabulate\", \"pandas\", \"seaborn\",\n",
    "        \"matplotlib\", \"tqdm\", \"mpld3\"])\n",
    "def download_and_preprocessing_dataset(\n",
    "    dataset_name: str, llm_model_pretrained: Input[Model],\n",
    "    dataset_preprocessed: Output[Dataset], dataset_visualization: Output[Markdown], dataset_metrics: Output[HTML]):\n",
    "    \n",
    "    import os  \n",
    "    import zipfile\n",
    "    import torch\n",
    "\n",
    "    import logging\n",
    "    \n",
    "    import mpld3\n",
    "\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt  \n",
    "    import seaborn as sns \n",
    "    \n",
    "    from transformers import AutoModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    from datasets import load_dataset\n",
    "    from datasets import concatenate_datasets\n",
    "    from datasets import Dataset\n",
    "    from datasets import DatasetDict\n",
    "\n",
    "    from sklearn.model_selection import train_test_split \n",
    "    \n",
    "    text_column        = \"Tweet text\"\n",
    "    label_column       = \"text_label\"\n",
    "    max_length         = 64                           # max number of tokens per example\n",
    "    \n",
    "    logger = logging.getLogger('kfp_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    def zip_folder(folder, zip_name):  \n",
    "        # Create a ZipFile object  \n",
    "        with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:  \n",
    "            # Walk through all files and directories in the specified folder  \n",
    "            for root, dirs, files in os.walk(folder):  \n",
    "                for file in files:  \n",
    "                    # Create the complete file path  \n",
    "                    file_path = os.path.join(root, file)  \n",
    "                    # Add the file to the ZIP file, preserving the folder structure  \n",
    "                    zipf.write(file_path, os.path.relpath(file_path, os.path.dirname(folder)))\n",
    "\n",
    "    def unzip_file(zip_file, extract_to):  \n",
    "        # Create a ZipFile object  \n",
    "        with zipfile.ZipFile(zip_file, 'r') as zipf:  \n",
    "            # Extract all the contents into the specified directory  \n",
    "            zipf.extractall(extract_to)\n",
    "            \n",
    "    unzip_file(llm_model_pretrained.path, \"/tmp\")\n",
    "    \n",
    "    model_path = \"/tmp/{revision}\".format(**{\"revision\": llm_model_pretrained.metadata[\"revision\"]})\n",
    "    \n",
    "    logger.info(\"snapshot\")\n",
    "    for root, dirs, files in os.walk(model_path):  \n",
    "        for file in files:  \n",
    "            logger.info(f\" - {file}\")\n",
    "    \n",
    "    source    = \"https://raw.githubusercontent.com/lowlevel-1989/twitter_complaints_spanish/master/dataset.csv\"\n",
    "    df = pd.read_csv(source, sep=\";\", encoding=\"utf-8\", names=[text_column, label_column])\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[label_column])\n",
    "\n",
    "    dataset_train = Dataset.from_pandas(train_df)\n",
    "    dataset_eval  = Dataset.from_pandas(test_df)\n",
    "\n",
    "    dataset = DatasetDict({\"train\": dataset_train, \"test\": dataset_eval})\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "\n",
    "    classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "    )\n",
    "    \n",
    "    dataset[\"train\"] = concatenate_datasets([dataset[\"train\"], dataset[\"test\"].shuffle().select(range(50))])\n",
    "    dataset[\"test\"]  = dataset[\"test\"].shuffle().select(range(100))\n",
    "    \"\"\"\n",
    "    \n",
    "    # pad_token_id: Se utiliza para rellenar (pad) secuencias de texto para que todas tengan la misma longitud dentro de un lote (batch) durante el entrenamiento.\n",
    "    # eos_token_id: Marca el final de una secuencia de texto.\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        batch_size = len(examples[text_column])\n",
    "\n",
    "        # Se crea este formato en el input prompt\n",
    "        # Tweet text : tweet : Label : \n",
    "        inputs     = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "\n",
    "        # Se deja el target original, pero pasado a str\n",
    "        targets    = [str(x) for x in examples[label_column]]\n",
    "\n",
    "        # tokerizamos los dataset\n",
    "        model_inputs = tokenizer(inputs)\n",
    "\n",
    "        # tokerizamos los targets\n",
    "        labels = tokenizer(targets)\n",
    "\n",
    "        # ajustamos los token para el input y el target, se explica en cada paso\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # asignamos el identicador del token para el dataset\n",
    "            sample_input_ids                  = model_inputs[\"input_ids\"][i]\n",
    "\n",
    "            # asingamos el identicador del token para el target\n",
    "            label_input_ids                   = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "\n",
    "            # el input sera tanto la entrada como la salida en vector\n",
    "            model_inputs[\"input_ids\"][i]      = sample_input_ids + label_input_ids\n",
    "\n",
    "            # asignamos la mascara 1 para los valores reales.\n",
    "            model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "\n",
    "            # rellenamos los ids del labels (targets) con un token -100 (token ignorado)\n",
    "            # contatenamos los target tokens al final\n",
    "            labels[\"input_ids\"][i]            = [-100] * len(sample_input_ids) + label_input_ids\n",
    "\n",
    "        # asignamos el relleno\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            sample_input_ids     = model_inputs[\"input_ids\"][i]\n",
    "            label_input_ids      = labels[\"input_ids\"][i]\n",
    "\n",
    "            # Asignamos el relleno al max_lenght, la ia funciona mejor\n",
    "            # cuando todos los parametros son del mismo tamaño\n",
    "            model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "                max_length - len(sample_input_ids)\n",
    "            ) + sample_input_ids\n",
    "\n",
    "            # asignamos la mascara 0 para los valores de relleno.\n",
    "            model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "                \"attention_mask\"\n",
    "            ][i]\n",
    "\n",
    "            labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "\n",
    "            # lo transformamos a tensores de torch\n",
    "            model_inputs[\"input_ids\"][i]      = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "            model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "            labels[\"input_ids\"][i]            = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    processed_datasets = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    \n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset  = processed_datasets[\"test\"]\n",
    "    \n",
    "    train_dataset.save_to_disk(\"/tmp/dataset/train\")\n",
    "    eval_dataset.save_to_disk(\"/tmp/dataset/eval\")\n",
    "    \n",
    "    \n",
    "    model_dir = dataset_preprocessed.path\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    dataset_preprocessed.path = os.path.join(model_dir, \"dataset.zip\")\n",
    "    zip_folder(f\"/tmp/dataset\", dataset_preprocessed.path)\n",
    "    \n",
    "    with open(dataset_visualization.path, \"w\") as f:\n",
    "        f.write(dataset[\"train\"].to_pandas().to_markdown())\n",
    "    \n",
    "    # Crear una figura con dos ejes  \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))  \n",
    "\n",
    "    # Primer gráfico en el primer eje  \n",
    "    sns.histplot(pd.Categorical(dataset[\"train\"][label_column], [\"sin queja\", \"queja\"]), ax=axes[0])  \n",
    "    axes[0].set_title('Distribution - Train Data')\n",
    "    axes[0].patches[0].set_facecolor('skyblue')\n",
    "    axes[0].patches[1].set_facecolor('salmon')\n",
    "\n",
    "    # Segundo gráfico en el segundo eje  \n",
    "    sns.histplot(pd.Categorical(dataset[\"test\"][label_column], [\"sin queja\", \"queja\"]), ax=axes[1])  \n",
    "    axes[1].set_title('Distribution - Test Data')  \n",
    "    axes[1].patches[0].set_facecolor('skyblue')\n",
    "    axes[1].patches[1].set_facecolor('salmon')\n",
    "\n",
    "    # Ajustar el layout  \n",
    "    plt.tight_layout() \n",
    "    \n",
    "    mpld3.save_html(fig, dataset_metrics.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "35024a99-f52a-4497-8a8f-1413d1ae3e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow:latest-gpu\", packages_to_install=[\n",
    "    \"torch\", \"transformers\", \"peft\", \"datasets\", \"tqdm\", \"matplotlib\", \"mpld3\"])\n",
    "def prompt_tuning_bloom(\n",
    "    num_epochs: int, dataset_preprocessed: Input[Dataset], llm_model_pretrained: Input[Model],\n",
    "    llm_model_snapshot: Output[Model], perplexity_visualization: Output[HTML], loss_visualization: Output[HTML]):\n",
    "    \n",
    "    import os\n",
    "    import zipfile\n",
    "    import torch\n",
    "\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger('kfp_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    import mpld3\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    \n",
    "    from transformers import AutoModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    from transformers import default_data_collator\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    \n",
    "    from peft import TaskType\n",
    "    from peft import get_peft_model\n",
    "    from peft import PromptTuningInit\n",
    "    from peft import PromptTuningConfig\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    from datasets import load_from_disk\n",
    "    \n",
    "    lr                 = 3e-2                         # training learning rate\n",
    "    batch_size         = 8                            # number of examples per batch\n",
    "    \n",
    "    def zip_folder(folder, zip_name):  \n",
    "        # Create a ZipFile object  \n",
    "        with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:  \n",
    "            # Walk through all files and directories in the specified folder  \n",
    "            for root, dirs, files in os.walk(folder):  \n",
    "                for file in files:  \n",
    "                    # Create the complete file path  \n",
    "                    file_path = os.path.join(root, file)  \n",
    "                    # Add the file to the ZIP file, preserving the folder structure  \n",
    "                    zipf.write(file_path, os.path.relpath(file_path, os.path.dirname(folder)))\n",
    "    \n",
    "    def unzip_file(zip_file, extract_to):  \n",
    "        # Create a ZipFile object  \n",
    "        with zipfile.ZipFile(zip_file, 'r') as zipf:  \n",
    "            # Extract all the contents into the specified directory  \n",
    "            zipf.extractall(extract_to)\n",
    "            \n",
    "    unzip_file(llm_model_pretrained.path, \"/tmp\")\n",
    "    unzip_file(dataset_preprocessed.path, \"/tmp\")\n",
    "    \n",
    "    model_path = \"/tmp/{revision}\".format(**{\"revision\": llm_model_pretrained.metadata[\"revision\"]})\n",
    "    \n",
    "    logger.info(\"snapshot\")\n",
    "    for root, dirs, files in os.walk(model_path):  \n",
    "        for file in files:  \n",
    "            logger.info(f\" - {file}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    train_dataset = load_from_disk(\"/tmp/dataset/train\")\n",
    "    eval_dataset  = load_from_disk(\"/tmp/dataset/eval\")\n",
    "    \n",
    "    logger.info(\"dataset\")\n",
    "    logger.info(train_dataset)\n",
    "    logger.info(eval_dataset)\n",
    "    \n",
    "    #  Set pin_memory=True to speed up the data transfer to the GPU during training, False for CPU\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True,            collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    "    )\n",
    "    eval_dataloader  = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n",
    "    \n",
    "    peft_config = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "        num_virtual_tokens=8,\n",
    "        prompt_tuning_init_text=\"Classify if the tweet is a complaint or not in spanish:\", # prompt inicial\n",
    "        tokenizer_name_or_path=model_path,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Setup an optimizer and learning rate scheduler:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=(len(train_dataloader) * num_epochs),\n",
    "    )\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(f\"device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    epochs                = []\n",
    "    train_ppl_list        = []\n",
    "    eval_ppl_list         = []\n",
    "    train_epoch_loss_list = []\n",
    "    eval_epoch_loss_list  = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.detach().float()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_preds = []\n",
    "        for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.detach().float()\n",
    "            eval_preds.extend(\n",
    "                tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "            )\n",
    "\n",
    "        eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "        eval_ppl = torch.exp(eval_epoch_loss)\n",
    "        train_epoch_loss = total_loss / len(train_dataloader)\n",
    "        train_ppl = torch.exp(train_epoch_loss)\n",
    "        \n",
    "        epochs.append(int(epoch))\n",
    "        train_ppl_list.append(float(train_ppl))\n",
    "        eval_ppl_list.append(float(eval_ppl))\n",
    "        train_epoch_loss_list.append(float(train_epoch_loss))\n",
    "        eval_epoch_loss_list.append(float(eval_epoch_loss))\n",
    "        \n",
    "        logger.info(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    ax1.plot(epochs, train_ppl_list, label='Train Perplexity')\n",
    "    ax1.plot(epochs, eval_ppl_list, label='Eval Perplexity')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Perplexity')\n",
    "    ax1.set_title('Training and Evaluation Perplexity')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    mpld3.save_html(fig1, perplexity_visualization.path)\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    ax2.plot(epochs, train_epoch_loss_list, label='Train Loss')\n",
    "    ax2.plot(epochs, eval_epoch_loss_list, label='Eval Loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Training and Evaluation Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    mpld3.save_html(fig2, loss_visualization.path)\n",
    "    \n",
    "    model_dir = llm_model_snapshot.path\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    llm_model_snapshot.path = os.path.join(model_dir, \"model.zip\")\n",
    "    model.save_pretrained(\"/tmp/model/snapshot\")\n",
    "    \n",
    "    zip_folder(\"/tmp/model/snapshot\", llm_model_snapshot.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "f2af00ba-b73b-4f33-a764-f3c289c1180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=[\"torch-model-archiver\"])\n",
    "def torch_model_archiver(\n",
    "    llm_model_pretrained: Input[Model], llm_model_snapshot: Input[Model], llm_model_archiver: Output[Model]):\n",
    "    \n",
    "    import os\n",
    "    import subprocess\n",
    "    import json\n",
    "    import logging\n",
    "    \n",
    "    from urllib import request\n",
    "    \n",
    "    logger = logging.getLogger('kfp_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    url = \"https://raw.githubusercontent.com/lowlevel-1989/bloomz/master/custom_handler.py\"\n",
    "\n",
    "    try:\n",
    "        request.urlretrieve(url, \"/tmp/custom_handler.py\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    # TODO: pasar por argumentos model_revision, text_column\n",
    "    config = {  \n",
    "        \"model_pretrained\": os.path.basename(llm_model_pretrained.path),  \n",
    "        \"model_snapshot\":   os.path.basename(llm_model_snapshot.path),\n",
    "        \"model_revision\":   \"a2845d7e13dd12efae154a9f1c63fcc2e0cc4b05\",\n",
    "        \"text_column\":      \"Tweet text\",\n",
    "    }\n",
    "    \n",
    "    with open(\"/tmp/setup_config.json\", \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "    \n",
    "    with open(\"/tmp/requirements.txt\", \"w\") as f:\n",
    "        f.write(\"peft\")\n",
    "    \n",
    "    logger.info(\"tmp\")\n",
    "    for root, dirs, files in os.walk(\"/tmp\"):  \n",
    "        for file in files:  \n",
    "            logger.info(f\" - {file}\")\n",
    "    \n",
    "    # Comando para crear el modelo con la metadata \n",
    "    command = [  \n",
    "        \"torch-model-archiver\",\n",
    "        \"--model-name\", \"bloomz\",\n",
    "        \"--version\", \"1.0\",\n",
    "        \"--handler\", \"/tmp/custom_handler.py\",\n",
    "        \"--extra-files\", \"{},{},/tmp/setup_config.json\".format(llm_model_pretrained.path, llm_model_snapshot.path),\n",
    "        \"-r/tmp/requirements.txt\"\n",
    "    ]\n",
    "\n",
    "    logger.info(config)\n",
    "    logger.info(command)\n",
    "    \n",
    "    try:   \n",
    "        subprocess.run(command, check=True)\n",
    "    except subprocess.CalledProcessError as e:  \n",
    "        raise e\n",
    "    \n",
    "    model_dir = llm_model_archiver.path\n",
    "    os.makedirs(f\"{model_dir}/model-store\", exist_ok=True)\n",
    "    os.makedirs(f\"{model_dir}/config\",      exist_ok=True)\n",
    "    \n",
    "    properties = {\n",
    "        \"inference_address\":        \"http://127.0.0.1:8083\",\n",
    "        \"management_address\":       \"http://127.0.0.1:8084\",\n",
    "        \"metrics_address\":          \"http://127.0.0.1:8085\",\n",
    "        \"grpc_inference_port\":      7072,\n",
    "        \"model_store\":              \"/mnt/models/model-store\",\n",
    "        \"install_py_dep_per_model\": \"true\",\n",
    "        \"model_snapshot\":      {\n",
    "            \"name\": \"startup.cfg\",\n",
    "            \"modelCount\": 1,\n",
    "            \"models\": {\n",
    "                \"bloomz\": {\n",
    "                    \"1.0\": {\n",
    "                        \"defaultVersion\":  \"true\",\n",
    "                        \"marName\": \"bloomz.mar\",\n",
    "                        \"minWorkers\":         1,\n",
    "                        \"maxWorkers\":         1,\n",
    "                        \"batchSize\":          1,\n",
    "                        \"maxBatchDelay\":    100,\n",
    "                        \"responseTimeout\":  120\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    logger.info(properties)\n",
    "\n",
    "    o = \"\"\n",
    "    for k, v in properties.items():\n",
    "        if k == \"model_snapshot\":\n",
    "            o = \"{}\\n{}={}\".format(o, k, json.dumps(v))\n",
    "            continue\n",
    "            \n",
    "        o = f\"{o}\\n{k}={v}\"\n",
    "    \n",
    "    properties_text = o.replace('\"true\"', \"true\").replace(\"'false'\", \"false\")\n",
    "    \n",
    "    logger.info(properties_text)\n",
    "    \n",
    "    with open(f\"{model_dir}/config/config.properties\", \"w\") as f:\n",
    "        f.write(properties_text)\n",
    "    \n",
    "    os.rename(\"bloomz.mar\", os.path.join(f\"{model_dir}/model-store\", \"bloomz.mar\"))\n",
    "    \n",
    "    llm_model_archiver.metadata[\"name\"]  = \"bloomz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "89ab3df4-956d-4559-9443-677ec23ba9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"tensorflow/tensorflow\", packages_to_install=['kserve', 'kubernetes'])\n",
    "def model_serving(llm_model_archiver : Input[Model]):\n",
    "    import time\n",
    "    import logging\n",
    "    \n",
    "    from kubernetes import client\n",
    "    from kubernetes import config\n",
    "    \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TorchServeSpec\n",
    "    \n",
    "    logger = logging.getLogger('kfp_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    name      = llm_model_archiver.metadata[\"name\"]\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    \n",
    "    kserve_version='v1beta1'\n",
    "    api_version = \"{}/{}\".format(constants.KSERVE_GROUP, kserve_version)\n",
    "    \n",
    "    uri = llm_model_archiver.uri.replace(\"minio\", \"s3\")\n",
    "    # uri = uri.rsplit(\"/\", 1)[0]\n",
    "    \n",
    "    logger.info(f\"{namespace}, {name}\")\n",
    "    logger.info(f\"{api_version}\")\n",
    "    logger.info(f\"{uri}\")\n",
    "    \n",
    "    \n",
    "    isvc = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "            spec=V1beta1InferenceServiceSpec(\n",
    "                predictor=V1beta1PredictorSpec(\n",
    "                    service_account_name=\"sa-minio-kserve\",\n",
    "                    pytorch=(\n",
    "                        V1beta1TorchServeSpec(\n",
    "                            args=[\n",
    "                                \"--no-config-snapshots\",\n",
    "                                \"--models bloomz=bloomz.mar\",\n",
    "                            ],\n",
    "                            storage_uri=uri,\n",
    "                            protocol_version=\"v2\",\n",
    "                            resources=client.V1ResourceRequirements(\n",
    "                                requests={\n",
    "                                    \"cpu\": \"1\",\n",
    "                                    \"memory\": \"2Gi\"},\n",
    "                                limits={\n",
    "                                    \"cpu\": \"1\",\n",
    "                                    \"memory\": \"8Gi\"},\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "d2c0b7c7-0908-4855-85ee-ab62eb453bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"tweet-classifier-dev\",\n",
    "    description=\"LLM\")\n",
    "def llm_pipeline(\n",
    "    hug_model_name :str  =\"bigscience/bloomz-560m\",\n",
    "    hug_model_revision :str =\"a2845d7e13dd12efae154a9f1c63fcc2e0cc4b05\",\n",
    "    dataset_name :str = \"twitter_complaints\",\n",
    "    num_epochs :int =50,\n",
    "    skip_model_serving :bool =False\n",
    "):\n",
    "    download_model_pretrained_task = download_model_pretrained(\n",
    "        hug_model_name=hug_model_name,\n",
    "        hug_model_revision=hug_model_revision\n",
    "    ) \\\n",
    "    .set_caching_options(enable_caching=True)\n",
    "    \n",
    "    download_and_preprocessing_dataset_task = download_and_preprocessing_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        llm_model_pretrained=download_model_pretrained_task.outputs[\"llm_model_pretrained\"],\n",
    "    ) \\\n",
    "    .set_caching_options(enable_caching=True)\n",
    "    \n",
    "    prompt_tuning_bloom_task = prompt_tuning_bloom(\n",
    "        num_epochs=num_epochs,\n",
    "        dataset_preprocessed=download_and_preprocessing_dataset_task.outputs[\"dataset_preprocessed\"],\n",
    "        llm_model_pretrained=download_model_pretrained_task.outputs[\"llm_model_pretrained\"]\n",
    "    ) \\\n",
    "    .set_accelerator_type(accelerator=\"nvidia.com/gpu\") \\\n",
    "    .set_accelerator_limit('1')                         \\\n",
    "    .set_caching_options(enable_caching=True)\n",
    "    \n",
    "    torch_model_archiver_task = torch_model_archiver(\n",
    "        llm_model_pretrained=download_model_pretrained_task.outputs[\"llm_model_pretrained\"],\n",
    "        llm_model_snapshot=prompt_tuning_bloom_task.outputs[\"llm_model_snapshot\"]\n",
    "    ).set_caching_options(enable_caching=True)\n",
    "    \n",
    "    with dsl.If(skip_model_serving == False):\n",
    "        model_serving_task = model_serving(\n",
    "            llm_model_archiver=torch_model_archiver_task.outputs[\"llm_model_archiver\"]\n",
    "        ) \\\n",
    "        .set_caching_options(enable_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "b4c2306d-2712-4166-a6eb-ad8f48094dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(llm_pipeline, \"pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "00426dae-7803-4d5c-b8c8-eb551c276d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "credentials = kfp.client.ServiceAccountTokenVolumeCredentials()\n",
    "\n",
    "client = kfp.Client(credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "8d716ec4-3f0b-44e1-9dd3-03b266ca7627",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/2127be0a-8a32-46fe-a018-e491a7da4cc5\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/0aed8a75-60ae-4355-bf7d-c58f521949a3\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.create_run_from_pipeline_package(\n",
    "    run_name=\"Tweet Classifier\",\n",
    "    experiment_name=\"LLM Tweet Classifier\",\n",
    "    pipeline_file=\"pipeline.yaml\",\n",
    "    arguments={\n",
    "        \"hug_model_name\":     \"bigscience/bloomz-560m\",\n",
    "        \"hug_model_revision\": \"a2845d7e13dd12efae154a9f1c63fcc2e0cc4b05\",\n",
    "        \"dataset_name\":       \"twitter_complaints\",\n",
    "        \"num_epochs\":         50\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077cab92-af2d-4a15-9062-6d3a38580abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff46329-d503-4924-b4ae-6acafaffbb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
